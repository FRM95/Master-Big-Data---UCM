---
title: 'Módulo Minería de datos: Series Temporales'
author: "Miguel Moreno Mardones"
date: "8 de Marzo de 2022"
output:
  word_document: default
  html_document: default
subtitle: 'Master Big Data: Data Science - UCM'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##Introducción

En el presente documento trabajaremos el concepto de las series temporales y la construcción de modelos de predicción. Una serie temporal de datos representa medidas normalmente cronológicas de un conjunto de datos cualquiera, por ejemplo: peso de una persona a lo largo de un tiempo, consumo de gas mensual de un territorio, ventas diarias de vehículos de una determinada compañía...

El objetivo por lo tanto no es otro que establecer un análisis de dicha serie temporal que nos permita interpretarla para, posteriormente, realizar un pronóstico de sus datos. Para comenzar con este ejercicio, necesitaremos encontrar un conjunto de datos que contenga una serie temporal de aproximadamente 150 observaciones y con tendencia estacionaria. 

##Presentación de la serie temporal 

La serie temporal escogida recoge el índice de producción industrial (IPI) en EEUU, el cual mide los cambios en volúmenes de producción y capacidad de las industrias manufacturera, minera, eléctrica, gas.. asociadas en un país. Son las empresas las que recopilan sus datos midiendo este volumen de producción, teniendo en cuenta las materias primas, energías utilizadas y la mano de obra empleada, tras esto, se realiza una recopilación nacional de todas estas empresas y se calcula dicho índice. 

En otros términos, indica la fuerza del sector industrial, ya que muestra información sobre cantidad de producción y ganancias respecto a los costes de producción. Un indice positivo del IPI tendrá buena repercusión en las empresas, en la moneda de un país... Analistas económicos suelen utilizar por tanto este índice para hacer pronósticos del PIB de un país. 
Nos centramos concretamente en el nivel de IPI mensual del valor IPI de las empresas cotizantes en Estados Unidos. Estos datos son publicados por el Consejo de la Reserva Federal US (FRB), que publica el IPI a mediados de cada mes así como una revisión de las estimaciones anteriores a finales de cada mes de marzo. Es también común interpretar este valor de IPI como el % de cambio mensual en vez de su valor de índice. Nos decantamos por el índice del IPI sin ajustes estacionales.  

Originalmente, el dataset se compone de observaciones datadas desde 1919 hasta 2022 a través de dos variables, donde se expone la Fecha (YY-MM-DD) y el valor del IPI mensual. Debido a la extensión del conjunto, recogeremos únicamente los valores comprendidos en el rango 2000-01-01 y 2022-01-01, ambos inclusive. Por lo que nuestra serie temporal se compondrá de 265 observaciones.

El conjunto de datos puede descargarse a través de la siguiente URL https://fred.stlouisfed.org/series/IPB50001N. 

```{r carga de paquetes, include=FALSE}
# Cargar librerías (paquete pacman)
#install.packages('pacman')
source('Funciones_R.R')
#pacman::p_load(readxl,fpp2,tseries,forecast,ggplot2,seasonal,descomponer,TSA)
paquetes(c('pacman','readxl','fpp2','tseries','forecast','ggplot2','seasonal','descomponer','TSA'))
#Cargamos el dataset
ipi_USA <- read.csv("IPB50001N.csv")
```

##Representacion gráfica y descomposicion de la serie

Representamos la serie temporal escogida mediante la función plot o autoplot, pero antes debemos transformarla a objeto de serie temporal ya que se trata de un dataset donde una variable pertenece a la fecha y otra al valor que toma el IPI. Observamos dos caídas de este valor debidas tanto a la crisis económica de 2008 y a la pandemia producida por la COVID-19, pero pese a esto, podríamos decir que el valor del IPI tiene una tendencia al alza a lo largo del eje temporal. 

```{r representacion de la serie}
#Lo convertimos a serie temporal y la representamos
ipi_USA <- ts(ipi_USA$IPB50001N, start=c(2000,1), frequency=12)
plot(ipi_USA, xlab = 'Year', ylab = 'IPI USA') + theme_minimal()
abline(h=mean(ipi_USA), col = 'blue')
```
Seguidamente, podemos predecir un comportamiento no estacionario, ya que no parece que la media y la varianza presente un comportamiento constante a lo largo de los años, existe una cierta dispersión del valor del IPI en el eje temporal, con bajadas y subidas. Esto sumado a la posible tendencia al alza nos hace suponer dicha predicción.

Realizamos el test ADF para comprobar la estacionaridad de la serie. La hipótesis nula establece la existencia de una raíz unitaria (serie no estacionaria), mientras que la hipótesis alternativa establece la no existencia de raíz unitaria (serie estacionaria).

```{r es estacionaria?}
#Aplicamos el adf.test para comprobar la estacionaridad
adf.test(ipi_USA)
```

El p-value obtenido en el adf.test() es > 0.05, lo que indica que hay evidencia para no rechazar la hipótesis nula. Por lo que definimos que la serie en este caso NO sería estacionaria.

Aparentemente presenta un patrón estacional, ya que en la gráfica observamos aumentos del valor del IPI de manera periódica a lo largo de los años. Para observar de mejor manera esto, realizamos una representación más reducida que recoja unos años del comportamiento (157 observaciones). 

```{r es estacional}
#Comportamiento estacional
#Reduccion de las observaciones (enero 2008 - enero 2021)
ipi_USARed <- window(x = ipi_USA, start=c(2008,1), frequency=12)
ipi_USARed <- window(x = ipi_USARed, end = c(2021,1), frequency=12)
autoplot(ipi_USARed, xlab = 'Year', ylab = 'IPI USA')
```
En el gráfico del valor del IPI se observa claramente la tendencia al alza comentada anteriormente además del comportamiento estacional de la serie, ya que muestra subidas del valor del IPI durante un periodo similar que se repite a lo largo de los años. 

De igual manera, descomponemos la serie temporal y la representamos de nuevo. Observamos la tendencia aparentemente al alza a lo largo de los años. Esta tendencia puede parecer inexistente debido a la cantidad de observaciones, luego reduciremos esta cantidad.

```{r descomposicion}
#Descomponemos la serie temporal ya que es estacional
ipi_USA_desc <- decompose(ipi_USARed, type = 'multiplicative')
autoplot(ipi_USA_desc)
ipi_USA_desc$figure
```

Además, aplicamos la función del periodograma para observar los posibles picos que expliquen la variabilidad de nuestra serie (un pico considerable en un periodo indicaría el comportamiento estacional de la serie en ese momento temporal).

```{r periodograma}
gperiodograma(ipi_USARed)
```
Observamos que no existen aumentos considerables de la densidad para ningun valor de la frecuencia (eje X), salvo dos picos en baja frecuencia. Con esto podríamos considerar que la serie no tiene comportamiento estacional, pero consideramos que si se hay evidencia de un aumento del valor del IPI durante algunos meses, aunque quizás no demasiado significativo. Represetamos la serie mucho más reducida (61 observaciones), de 2014 a 2019. 

```{r observamos picos estacionales }
aux <- window(x = ipi_USA, start=c(2014,1), frequency=12)
aux <- window(x = aux, end = c(2019,1))
autoplot(aux, xlab = 'Year', ylab = 'IPI USA')
rm(aux)
```
Vemos por lo tanto la existencia de dos momentos a lo largo del año donde este valor aumenta. Este suceso se repite durante la serie temporal y concuerda con antes de la mitad del año y justamente después.

Concluimos este apartado reservando aproximadamente 10 de las últimas observaciones de la serie temporal original o algunos de los últimos periodos (febrero 2021 a enero 2022).Para así poder comparar con las predicciones realizadas posteriormente en cada uno de los modelos que generemos. 

```{r reservamos valores para test}
#Ultimas observaciones reales (febrero 2021 - enero 2022)
ipi_USAPred <- window(x = ipi_USA, start=c(2021,2), frequency=12)
```

##Modelo suvizado

Es hora de escoger un modelo de suavizado en base a las condiciones de nuestra serie temporal. Ya que nuestra serie muestra una tendencia al alza, podríamos escoger el suavizado mediante el método de alisado simple o alisado doble de holt. 

No obstante, ya que presenta aparentemente una pequeña componente de estacionalidad que ha de ser modelada, damos por valido utilizar el alisado Holt-Winters aditivo (dicha componente estacional es constante en el tiempo). Nos decantamos finalmente por el alisado alisado Holt-Winters aditivo.

```{r suavizado }
#Alisado mediante Holt Winters aditivo
x1 <- hw(ipi_USARed, h=12,level = c(80, 95))
# Inspección del objeto creado y Distribución de residuos
print(x1)
x1$model
autoplot(x1$residuals)
#Visualización
autoplot(x1) + autolayer(fitted(x1), series="Fitted") + autolayer(ipi_USAPred, series="Test") +
  ylab("IPI USA") + xlab("Year")

ipi_USAPred
```

La predicción realizada por el Forecast de Holt Winters no se aleja mucho de los valores presentados en las observaciones "Test". Con un intervalo de confianza de 95%, observamos las columnas de Lo y Hi y las comparamos con los valores extraidos como ipi_USAPred.


##Correlogramas

Comprobar si la evolución de los datos de la serie temporal poseen relación con su comportamiento del pasado puede indicarnos la estacionaridad (ACF) o el orden del modelo ARIMA a utilizar (PACF). Es así como nace el concepto de correlograma, donde se ven las autocorrelaciones entre un valor x y su valor x+t, siendo t un comportamiento futuro de la misma variable x.

La función de correlación simple ACF, mide la correlación de variables separadas por esas k posiciones entre x y x+t, mientras que la función PACF mide esa misma correlación de variables separadas por k posiciones, pero eliminando los efectos de las posibles k posiciones intermedias. 

```{r correlogramas}
#Mediante ACF podemos comprobar estacionalidad 
ggAcf(ipi_USARed, plot = FALSE)
#Mediante PACF podemos comprobar el orden de los modelos ARIMA 
ggPacf(ipi_USARed, plot = FALSE)
```

Debemos interpretar los correlogramas para entender como se comporta nuestra serie y observar las correlaciones entre datos de nuestra serie. El eje X de los correlogramas mide el retardo entre observaciones, si x = 1, el retardo es mensual (entre enero y febrero), si x = 3 es trimestral (entre enero y abril), si x = 12 es anual (enero 2021 y enero 2020). 

##Correlograma ACF

Tenemos el mayor indice de correlación positiva con un retardo mensual (+0.9 aproximadamente). Tras esto, los valores de correlación decrecen de manera progresiva siendo significativamente mayores que cero. Esto es indicador de una serie no estacionaria como vimos anteriormente. 

##Correlograma PACF

En este correlograma encontramos de nuevo el pico de correlación positivo significativo entre meses consecutivos, similar al gráfico ACF ya que no hay posibles correlaciones entre meses intermedios. Tras esto no encontramos correlación entre meses salvo el retraso 13, donde existe una correlación negativa pero muy poco significativa, aproximadamente -0.3. Se observa una disminución del índice de correlación conforme avanza el eje X al no intervenir meses intermedios en el retardo. También existen dos niveles de correlaciones parciales positivas en los retrasos 9 y 12, pero muy poco significativas.

Según los correlogramas, podemos apreciar un patrón AR(1), modelo AR de orden 1. Ya que el nivel de significación para un retardo o lag de X = 1 es el mayor de los ejes, seguido de un retardo negativo pero muy poco significativo (valor menor a -0.25 en PACF). En los correlogramas diferenciados, volvemos a encontrar esta significación para X = 1, y un nivel negativo de significación para el retaso de X = 2. Por lo que el valor del IPI en un instante t, quizás dependa de los tiempos anteriores t-1 y t-2. De ahí todos los valores positivos en el correlograma ACF pero con tendencia decreciente, si aumentamos el retraso, disminuye la correlación.

##Camino hacia la estacionariedad

Ya que nuestra serie no es estacionaria, aplicamos un grado de diferenciación para comprobar si nuestra serie se adquiere dicha característica. Calculamos entonces el número de diferenciaciones regulares y estacionales necesarios para convertirla a estacionaria.

```{r corr diferenciada}
#Calculamos numero de diferenciaciones regulares para que nuestra serie sea estacionaria
ndiffs(ipi_USARed)
#Calculamos numero de diferenciaciones estacionales para que nuestra serie sea estacionaria
nsdiffs(ipi_USARed)

#Representacion de la serie diferenciada
autoplot(diff(ipi_USARed)) + ylab("IPI USA Diferenciada") + xlab("Year")

#Comprobamos estacionaridad de la serie diferenciada
adf.test(diff(ipi_USARed))

#Mediante ACF podemos comprobar estacionalidad 
ggAcf(diff(ipi_USARed), plot = FALSE)
#Mediante PACF podemos comprobar el orden de los modelos ARIMA 
ggPacf(diff(ipi_USARed), plot = FALSE)
```

El valor del test ADF en la serie diferenciada obtiene un valor de p < 0.05. Por lo que en este caso tendríamos una serie estacionaria aplicando el grado de diferenciacion 1. En el ACF de la serie diferenciada, obtenemos un retraso de X = 12 donde encontramos correlación positiva (poco significativa), desaparece el decrecimiento lento. En PACF la correlación positiva se repite para el mismo retraso temporal X = 12. Si bien antes de diferenciar la serie, recogíamos un posible orden = 1 para AR, en la serie diferenciada no observamos significancia que quizamos debamos incluir. 

Procederemos a realizar varios modelos ARIMA(p,d,q) intercalando varias posibilidades:

```{r modelos manuales}
#Generamos el primer modelo
modelo1 <- ipi_USARed %>%  Arima(order=c(1,1,0)) #Autorregresion de orden 1
modelo1$aic

#Generamos el tercer modelo
modelo2 <- ipi_USARed %>%  Arima(order=c(1,1,1)) #Grado de diferenciacion 1
modelo2$aic

#Modelo automatico ARIMA
modeloARIMA <- auto.arima(ipi_USARed, seasonal = FALSE)
modeloARIMA %>% residuals()  %>% ggtsdisplay() 
modeloARIMA$aic
```

El modelo que obtiene el menor valor AIC y es significativamente relevante (p-value < 0.05) es efectivamente el modelo automático o el modelo número 2 donde no interpretamos como válido el modelo autoregresivo. Por lo que será un modelo ARIMA(0,1,0) un gran candidato a escoger. No observamos información relevante en el gráfico PACF asociado a ningún retardo. 

##Validación del modelo

Ejecutamos el test de KPSS - Kwiatkowski-Phillips-Schmidt-Shin para comprobar de nuevo la estacionariedad de la serie temporal diferenciada (previamente el adf.test sobre esta serie diferenciada indicó que lo era). 

```{r test kpss}
#Test kpss
kpss.test(diff(ipi_USARed))
```

Este nuevo test de KPSS indica un p-value > 0.05. No hay evidencia para rechazar la hipótesis nula, por lo que seguimos teniendo una serie no estacionaria. Por esta razón es quizás necesario implementar una componente estacional en nuestro modelo, es decir, introducir un modelo SARIMA. Eso significa introducir a nuestros parámetros regulares (p,d,q) otra componente relacionada con las variaciones estacionales (P,D,Q).


```{r validacion}
#Modelo SARIMA
modeloSARIMA <- Arima(ipi_USARed,order=c(1,1,0),seasonal=c(0,0,2))
modeloSARIMA

modeloSARIMA_aut <- auto.arima(ipi_USARed, seasonal = TRUE)

#ACF y PACF
autoplot(acf(modeloSARIMA$residuals))
autoplot(pacf(modeloSARIMA$residuals))
```

El modelo automatico generado es SARIMA (0,1,0)(0,0,2) no llega a superar el Test de Shapiro-Wilk de normalidad, por lo que generamos un nuevo modelo SARIMA (1,1,0)(0,1,2). Sus gráficos de correlogramas de residuos no indican ningún retardo significativo, por lo estos residuos pueden parecerse a ruido blanco. Comprobamos finalmente la normalidad del modelo mediante el test de Q de Ljung-Box y a través de Shapiro-Wilk.

```{r Ljung-Box}
#Test Ljung-Box
independencia <- Box.test(modeloSARIMA$residuals, type="Ljung-Box")
independencia$p.value

#Representacion de los datos 
qqnorm(modeloSARIMA$residuals)
qqline(modeloSARIMA$residuals) 

#Test de Shapiro-Wilk
normalidad <-shapiro.test(modeloSARIMA$residuals)    
normalidad$p.value

```

El p-value para la prueba Q de Ljung-Box es superior a 0.05 (0.85), así como los puntos descansan sobre la línea recta del gráfico de Normal Q-Q Plot, por lo que se distribuyen de manera independiente. El valor de p-value de Shapiro-Wilk es < 0.05. Podemos decir que nuestro modelo SARIMA(1,1,0)(0,0,2) queda validado. La expresión algebraica para dicho modelo sería: 

##Predicción

Realizamos la predicción sobre nuestra serie temporal comparando los modelos generados. 

```{r prediccion }
#Prediccion automatico
predmodelo2 <- forecast(modelo2, h=12)
predmodelo2 %>% autoplot() + autolayer(ipi_USAPred, series = 'Real')

pred_SARIMA_aut <- forecast(modeloSARIMA_aut, h=12)
pred_SARIMA_aut %>% autoplot() + autolayer(ipi_USAPred, series = 'Real')

#Prediccion SARIMA
pred_SARIMA <- forecast(modeloSARIMA, h=12)
pred_SARIMA %>% autoplot() + autolayer(ipi_USAPred, series = 'Real')
```
##Conclusiones

Realizadas las predicciones, observamos que los modelos SARIMA generan una mejor predicción que los modelos ARIMA. Por lo que ha sido un acierto añadir la parte estacional. Si bien es cierto que dichos predicciones del IPI toman valores menores que los reales, forman la misma tendencia. Esto es debido a la caída del valor del IPI en 2020, que ha influenciado enormemente en la predicción utilizando dichos modelos ya que se trata de un valor muy atípico en la serie temporal. 

Cuando se eligió el ejemplo de serie temporal a escoger, numerosos lugares de referencia hablaban de la dificultad para predecir modelos no estacionales e incluso no estacionarios. Nuestro caso es este, y hemos comprobado que es así, ya que aun estableciendo un orden 1 debido al retraso en la autoregresión, un orden 1 para el paso de diferenciación a modelo estacionario, nuestro modelo no ha sido capaz de predecir correctamente los resultados. Holt Winters sin embargo ha realizado mejores predicciones a través del suavizado ya que no se ha visto tan influenciado por la caída. 
